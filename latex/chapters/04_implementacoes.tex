\chapter{Implementação}
\label{ch:implementation}

Nesse capítulo serão mostradas as implementações reais de cada
algoritmo, acrescidas de comentários que elucidem as escolhas de
implementações tomadas.

\section{Algoritmos de Árvore Geradora Mínima}

\subsection{Algoritmo de Kruskal}

\begin{listing}[H]
  \caption{Construtor do Iterador de Kruskal}
\begin{minted}{rust}
pub fn new(graph: &'a G) -> Self {
    let nodes: Vec<T> = graph.nodes().collect();
    let accepted_adj: HashMap<T, HashSet<T>> = HashMap::with_capacity(nodes.len());

    let mut seen: HashSet<(T, T)> = HashSet::with_capacity(nodes.len() * 2);
    let mut edges: Vec<(T, T, i32)> = Vec::new();

    for &u in &nodes {
        for (v, w) in graph.weighted_neighbors(u) {
            let (a, b) = if u <= v { (u, v) } else { (v, u) };
            if seen.insert((a, b)) {
                edges.push((a, b, w));
            }
        }
    }

    edges.sort_by(|(ua, va, wa), (ub, vb, wb)| {
        wa.cmp(wb).then_with(|| ua.cmp(ub)).then_with(|| va.cmp(vb))
    });

    KruskalIter {
        _graph: graph,
        edges,
        idx: 0,
        accepted_adj,
    }
}
\end{minted}
\end{listing}

\begin{listing}[H]
  \caption{Iteração de Kruskal}
\begin{minted}{rust}
impl<'a, T, G> Iterator for KruskalIter<'a, T, G>
where
    T: Node + Ord,
    G: UndirectedGraph<T> + WeightedGraph<T, i32> + ?Sized,
{
    type Item = KruskalEvent<T>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.idx < self.edges.len() {
            let (u, v, w) = self.edges[self.idx];
            self.idx += 1;

            if !self.connected_by_accepted(u, v) {
                self.accepted_adj.entry(u).or_default().insert(v);
                self.accepted_adj.entry(v).or_default().insert(u);
                Some(KruskalEvent::EdgeAdded((u, v, w)))
            } else {
                Some(KruskalEvent::EdgeSkipped((u, v, w)))
            }
        } else {
            None
        }
    }
}
\end{minted}
\end{listing}

A função \texttt{new} prepara previamente todos os dados necessários
para a execução incremental do Algoritmo de Kruskal. Ela coleta todas
as arestas do grafo, organiza‑as em um vetor e as ordena pelo peso,
garantindo que possam ser examinadas da menor para a maior. Também é
inicializada a estrutura \texttt{accepted\_adj}, que representa o
subgrafo formado apenas pelas arestas já selecionadas na construção da solução.

A lógica do algoritmo é realizada de forma incremental no método
\texttt{next()}. A cada chamada, o iterador processa a próxima aresta
da lista ordenada. Para decidir se ela deve ser incluída na árvore
geradora, o método verifica se seus vértices já estão conectados no
subgrafo parcial armazenado em \texttt{accepted\_adj}. Caso ainda
estejam conectados, isto é, caso a inclusão da aresta não forme um
ciclo, a aresta é incorporada à solução e registrada na estrutura
\texttt{accepted\_adj}.

Ao final das iterações, as arestas acumuladas em
\texttt{accepted\_adj} constituem o resultado do algoritmo.

\subsection{Algoritmo de Prim}

\begin{listing}[H]
  \caption{Construtor do Iterador de Prim}
\begin{minted}{rust}
pub fn new(graph: &'a G) -> Self {
    let nodes: Vec<T> = graph.nodes().collect();
    let mut visited: HashSet<T> = HashSet::with_capacity(nodes.len());
    let mut heap = BinaryHeap::new();

    if let Some(&s) = nodes.first() {
        visited.insert(s);
        for (v, w) in graph.weighted_neighbors(s) {
            let (a, b) = if s <= v { (s, v) } else { (v, s) };
            heap.push(Reverse((w, a, b)));
        }
    }

    PrimIter {
        _graph: graph,
        visited,
        heap,
        nodes_len: nodes.len(),
    }
}
\end{minted}
\end{listing}

\begin{listing}[H]
  \caption{Iteração de Prim}
\begin{minted}{rust}
impl<'a, T, G> Iterator for PrimIter<'a, T, G>
where
    T: Node + Ord,
    G: UndirectedGraph<T> + WeightedGraph<T, i32> + ?Sized,
{
    type Item = PrimEvent<T>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.visited.len() >= self.nodes_len {
            return None;
        }

        while let Some(Reverse((w, u, v))) = self.heap.pop() {
            let u_vis = self.visited.contains(&u);
            let v_vis = self.visited.contains(&v);

            if u_vis && v_vis {
                return Some(PrimEvent::EdgeSkipped(u, v, w));
            }

            if u_vis ^ v_vis {
                let new = if u_vis { v } else { u };
                let (a_out, b_out) = if u <= v { (u, v) } else { (v, u) };
                self.visited.insert(new);
                for (nv, w2) in self._graph.weighted_neighbors(new) {
                    let (aa, bb) = if new <= nv { (new, nv) } else { (nv, new) };
                    self.heap.push(Reverse((w2, aa, bb)));
                }
                return Some(PrimEvent::EdgeAdded(a_out, b_out, w));
            }

            continue;
        }

        None
    }
}
\end{minted}
\end{listing}

A função \texttt{new()} prepara previamente todos os dados
necessários para a execução incremental do Algoritmo de Prim.
Inicialmente, escolhe-se um vértice arbitrário como ponto de partida
(neste caso, o primeiro retornado por \texttt{graph.nodes()}). Esse
vértice é marcado como visitado e todas as suas arestas incidentes
são inseridas em um heap mínimo, que servirá como estrutura de
prioridade para selecionar sempre a próxima aresta de menor peso que
expande a árvore. São também inicializados os conjuntos
\texttt{visited} e \texttt{heap}, que representarão, respectivamente,
os vértices já incorporados à solução e o conjunto de arestas
candidatas que conectam o conjunto visitado ao restante do grafo.

A lógica do algoritmo é realizada de forma incremental no método
\texttt{next()}. A cada chamada, o iterador remove do heap a aresta
de menor peso disponível. Se essa aresta conecta um vértice já
visitado a um ainda não visitado, então ela é aceita na árvore
geradora mínima. O vértice recém‑incluído é adicionado a
\texttt{visited}, e todas as suas arestas são inseridas no heap,
permitindo que novas expansões sejam consideradas. Caso contrário, se
a aresta conecta dois vértices já visitados, ela é descartada, pois
sua inclusão criaria um ciclo.

Ao final das iterações, quando todos os vértices tiverem sido
incorporados ao conjunto \texttt{visited}, o método \texttt{next()}
deixa de produzir eventos e a execução se encerra. As arestas aceitas
ao longo do processo formam a árvore geradora mínima do grafo.

\section{Algoritmos de Caminho Mais Curto}

\subsection{Algoritmo de Dijkstra}
Para a implementação de Dijkstra foi elaborado a seguinte \textit{struct}:
\begin{listing}[H]
  \caption{Estrutura do iterador de Dijkstra}
\begin{minted}{rust}
struct DijkstraResult<Node, Weight> {
    route: HashMap<Node, (Weight, Option<Node>)>,
}
\end{minted}
\end{listing}

Esta estrutura é responsável por armazenar um dicionário que contém o
resultado do Algortimo de Dijkstra, onde cada chave é um nó que
aponta para uma dupla, que indica o predecessor até o nó e também a
distância dele da origem.

\begin{listing}[H]
  \caption{Implementação do Algoritmo de Dijkstra}
\begin{minted}{rust}
impl<N: Node, W: Weight> DijkstraResult<N, W> {
    pub fn new(graph: &(impl WeightedGraph<N, W> + ?Sized), start: N) -> Self {
        let mut route: HashMap<N, (W, Option<N>)> = HashMap::new();
        let mut visited: HashSet<N> = HashSet::new();
        let mut distance: HashMap<N, W> = HashMap::new();
        let mut pred: HashMap<N, Option<N>> = HashMap::new();
        distance.insert(start, W::zero());
        pred.insert(start, None);

        for (neighbor, weight) in graph.weighted_neighbors(start) {
            pred.insert(neighbor, Some(start));
            distance.insert(neighbor, weight);
        }

        loop {
            let mut unvisited_node: Option<(N, W)> = None;
            for node in graph.nodes() {
                if !visited.contains(&node)
                    && let Some(distance) = distance.get(&node)
                    && (unvisited_node.is_none()
                        || (unvisited_node.is_some() && distance < &unvisited_node.unwrap().1))
                {
                    unvisited_node = Some((node, *distance));
                }
            }

            match unvisited_node {
                None => break,
                Some((node, node_weight)) => {
                    visited.insert(node);

                    for (neighbor, weight) in graph.weighted_neighbors(node) {
                        if !visited.contains(&neighbor) {
                            let new_distance = weight + node_weight;

                            match distance.get(&neighbor) {
                                Some(&neighbor_distance) => {
                                    if neighbor_distance > new_distance {
                                        distance.insert(neighbor, new_distance);
                                        pred.insert(neighbor, Some(node));
                                    }
                                }
                                None => {
                                    distance.insert(neighbor, new_distance);
                                    pred.insert(neighbor, Some(node));
                                }
                            }
                        }
                    }

                    let mut parent: Option<N> = None;
                    if let Some(opt) = pred.get(&node) {
                        parent = *opt;
                    }

                    route.insert(node, (node_weight, parent));
                }
            }
        }
        Self { route }
    }
}
\end{minted}
\end{listing}

A função \textit{new} é responsável por executar o Algoritmo de
Dijkstra e retornar seu resultado. Para manter o controle de vértices
visitados, a distância até eles e também seus predecessores, criamos
dicionários e conjuntos auxiliares para este processo.

O algoritmo inicia definindo a distância e o predecessor do nó
inicial como 0 e \textit{None}, respectivamente, para então definir
os mesmos elementos para os seus vizinhos, mas sem marcar ninguém
como visitado. Após isso, inicia-se o loop principal: a cada
iteração, é buscado o vértice com menor distância, para que este seja
visitado e os seus vizinhos sejam relaxados, ou seja, tenham sua
distância e predecessor atualizados caso seja vantajoso; ao visitar
um vértice, note que ele é salvo no dicionário \texttt{route}, onde o
vértice é a chave que aponta para a dupla com a sua distância e
também seu predecessor. Ao acabar os nós não visitados, a função
retorna a rota completa.

Para encontrar, então, o caminho entre dois vértices, o consumidor da
função pode acessar o dicionário \texttt{route} a partir do vértice
final e ir explorando seus predecessores até encontrar o nó inicial.
Isso traz solidez e isolamento para o algoritmo, que é capaz de
cumprir com eficácia seu objetivo principal sem se preocupar com o
modo em que as informações serão usadas.

\subsection{Algoritmo de Bellman-Ford}
Já para a implementação de Bellman-Ford foi elaborado a seguinte
\textit{struct}:

\begin{listing}[H]
  \caption{Estrutura que encapsula o resultado do Algoritmo de Bellman-Ford}
\begin{minted}{rust}
pub struct BellmanFordResult<Node, Weight> {
    pub dist: HashMap<Node, Weight>,
    pub pred: HashMap<Node, Option<Node>>,
    pub has_negative_cycle: bool,
}
\end{minted}
\end{listing}

Tal estrutura contém três informações de relevância para a análise do
resultado do algoritmo:

\begin{itemize}
  \item \textbf{dist:} O dicionário que contém a relação de custo
    acumulado para percorrer o grafo até determinado vértice.
  \item \textbf{pred:} Dicionário que indica a ordem de precedência
    do melhor caminho dado um vértice inicial.
  \item \textbf{has-negative-cycle:} Um booleano correspondente há
    presença ou não de ciclo negativo no grafo.
\end{itemize}

E através dessa estrutura, é implementado o método \texttt{new} para
na interface correspondente:

\begin{listing}[H]
  \caption{Interface do algoritmo de Bellman-Ford}
\begin{minted}{rust}
impl<N: Node, W: Weight> BellmanFordResult<N, W> {
    fn new(g: &(impl WeightedGraph<N, W> + ?Sized), start: N) -> Self;
}
\end{minted}
\end{listing}

E segue a implementação do método:

\begin{listing}[H]
  \caption{Implementação do Algoritmo de Bellman-Ford}
\begin{minted}{rust}
    pub fn new(g: &(impl WeightedGraph<N, W> + ?Sized), start: N) -> Self {
        let mut dist = HashMap::new();
        let mut pred = HashMap::new();

        for n in g.nodes() {
            pred.insert(n, None);
            dist.insert(n, W::max_value());
        }

        dist.insert(start, W::zero());

        for _i in 1..g.order() {
            for out_node in g.nodes() {
                for (in_node, weight) in g.weighted_neighbors(out_node) {
                    let new_dist = dist[&out_node].saturating_add(&weight);

                    if dist[&in_node] > new_dist {
                        dist.insert(in_node, new_dist);
                        pred.insert(in_node, Some(out_node));
                    }
                }
            }
        }

        let mut has_negative_cycle = false;

        for out_node in g.nodes() {
            for (in_node, weight) in g.weighted_neighbors(out_node) {
                if dist[&in_node] > dist[&out_node].saturating_add(&weight) {
                    has_negative_cycle = true;
                }
            }
        }

        Self {
            dist,
            pred,
            has_negative_cycle,
        }
    }
\end{minted}
\end{listing}

A execução do Algoritmo de Bellman-Ford se dá através da chamada da
função \textit{new}. Nela, dado um grafo e um vértice inicial, o
algoritmo é excecutado e tem seu resultado encapsulado através do
\textit{BellmanFordResult}.
\subsection{Algoritmo de Floyd-Warshall}
\label{sec:code_floyd-warshall}

Para o algoritmo de
Floyd-Warshall~(\ref{sec:pseudocode_floyd-warshall}), inicialmente definimos uma
estrutura, que vai representar o retorno da função:

\begin{listing}[H]
  \caption{Estrutura de resultado do Algoritmo Floyd-Warshall}
\begin{minted}{rust}
struct FloydWarshallResult<Node, Weight> {
    dist: HashMap<Node, HashMap<Node, Weight>>,
    pred: HashMap<Node, HashMap<Node, Node>>,
}
\end{minted}
\end{listing}

Ela é genérica a qualquer tipo de vértice e de custos, identificados por
\texttt{Node} e \texttt{Weight}. \texttt{dist} representa a matriz de
distâncias do resultado do algoritmo e \texttt{pred} representa a
matriz de predecessores. Para o resultado ser genérico a qualquer
tipo, é necessário implementar a matriz através de tabelas hash que
indexam vértices e armazenam outras tabelas hash, as quais indexam vértices e
custo (no caso da matriz de distâncias). De forma que um acesso
\texttt{dist[i][j]} represente a
distância entre o vértice \texttt{i} e o vértice \texttt{j}.

Após definir a estrutura, definimos a interface do algoritmo como sendo o
construtor dessa estrutura, nomeado de \texttt{new}:
\begin{listing}[H]
  \caption{Interface do algoritmo de Floyd-Warshall}
  \label{code:interface_floydwarshall}
\begin{minted}{rust}
impl<N: Node, W: Weight> FloydWarshallResult<N, W> {
    fn new(g: &(impl WeightedGraph<N, W> + ?Sized)) -> Self;
}
\end{minted}
\end{listing}

O algoritmo espera a implementação de um grafo ponderado \texttt{g}
que pode ter seu tamanho conhecido ou não em tempo de compilação (com
a restrição de traço \texttt{?Sized}). \texttt{N} e \texttt{W} são o
tipo do vértice e do custo, e são restritos pelos traços
\texttt{Node} e \texttt{Weight} respectivamente.

Começando a implementação, definimos as duas estruturas que vão
constituir o resultado e as inicializamos:

\begin{minted}{rust}
let mut dist = HashMap::with_capacity(g.order());
let mut pred = HashMap::with_capacity(g.order());
for n in g.nodes() {
    let mut neighbors_dist = HashMap::new();
    let mut neighors_pred = HashMap::new();

    neighbors_dist.insert(n, W::zero());
    neighors_pred.insert(n, n);

    for (neighbor, weight) in g.weighted_neighbors(n) {
        neighbors_dist.insert(neighbor, weight);
        neighors_pred.insert(neighbor, n);
    }

    dist.insert(n, neighbors_dist);
    pred.insert(n, neighors_pred);
}
\end{minted}

Essa inicialização garante que os custos e predecessores dos vértices
adjacentes já sejam incorporados na estrutura. Entretanto, note que
as outras distâncias e predecessores não são inicializados, diferente
de como é descrito no
pseudocódigo~\ref{sec:pseudocode_floyd-warshall}. Note também que
explicitamente definimos \texttt{pred[i][i]} como \texttt{i} e
\texttt{dist[i][i]} como \texttt{0} nessa implementação. Fizemos isso
para manter a fidelidade a implementações mais clássicas do
algoritmo, mas não é obrigatório.

Antes de partir para o cerne da implementação, definimos uma função
anônima denominada \texttt{unwrap\_dist}, para lidar com o acesso
seguro as tabelas hash de rust, especificamente a \texttt{dist}. Pois
o acesso usando operador \texttt{[]} pode causar pânicos se a chave
não existir, o que no nosso caso não é garantido para o segundo acesso.

\begin{minted}{rust}
let unwrap_dist = |dist: &HashMap<N, HashMap<N, W>>, i, j| {
    dist[&i].get(&j).copied().unwrap_or(W::max_value())
};
\end{minted}

No caso em que a segunda chave não exista, retornamos o valor máximo do tipo
numérico do custo (\texttt{W::max\_value()}) ao invés do valor que
existiria no acesso.

Por fim, partimos para o cerne da implementação, o loop principal do algoritmo:

\begin{minted}{rust}
for k in g.nodes() {
    for i in g.nodes() {
        for j in g.nodes() {
            let dist_ik = unwrap_dist(&dist, i, k);
            let dist_kj = unwrap_dist(&dist, k, j);
            let dist_ij = unwrap_dist(&dist, i, j);
            if let Some(sum) = dist_ik.checked_add(&dist_kj)
                && sum < dist_ij
            {
                dist.entry(i).and_modify(|ds| {
                    ds.entry(j)
                        .and_modify(|d| *d = sum)
                        .or_insert(sum);
                });
                let pred_kj = pred[&k][&j];
                pred.entry(i).and_modify(|ps| {
                    ps.entry(j)
                        .and_modify(|p| *p = pred_kj)
                        .or_insert(pred_kj);
                });
            }
        }
    }
}

Self { dist, pred }
\end{minted}

A implementação segue o algoritmo clássico em semântica, entretanto,
com diversas checagens de segurança. Por exemplo, ao checar se
\texttt{k} melhora a distância entre \texttt{i} e \texttt{j}, é
primeiro verificado se a soma entre \texttt{dist[i][k]} e
\texttt{dist[k][j]} causa overflow de inteiro, usando o método
\texttt{checked\_add} de um tipo numérico:

\begin{minted}{rust}
if let Some(sum) = dist_ik.checked_add(&dist_kj)
    && sum < dist_ij { ... }
\end{minted}

Além disso, ao invés de alterar diretamente o predecessor e distância de
$ij$ quando é necessário, alteramos ou inserimos o novo valor, visto
que não é garantido que \texttt{pred[i][j]} ou \texttt{dist[i][j]}
existiam previamente.

\begin{minted}{rust}
dist.entry(i).and_modify(|ds| {
    ds.entry(j)
        .and_modify(|d| *d = sum)
        .or_insert(sum);
});
let pred_kj = pred[&k][&j];
pred.entry(i).and_modify(|ps| {
    ps.entry(j)
        .and_modify(|p| *p = pred_kj)
        .or_insert(pred_kj);
});
\end{minted}

A implementação completa do algoritmo é a seguinte:

\begin{listing}[H]
  \caption{Implementação do algoritmo de Floyd-Warshall}
  \begin{minted}{rust}
impl<N: Node, W: Weight> FloydWarshallResult<N, W> {
    pub fn new(g: &(impl WeightedGraph<N, W> + ?Sized)) -> Self {
        let mut dist = HashMap::with_capacity(g.order());
        let mut pred = HashMap::with_capacity(g.order());
        for n in g.nodes() {
            let mut neighbors_dist = HashMap::new();
            let mut neighors_pred = HashMap::new();

            neighbors_dist.insert(n, W::zero());
            neighors_pred.insert(n, n);

            for (neighbor, weight) in g.weighted_neighbors(n) {
                neighbors_dist.insert(neighbor, weight);
                neighors_pred.insert(neighbor, n);
            }

            dist.insert(n, neighbors_dist);
            pred.insert(n, neighors_pred);
        }

        let unwrap_dist = |dist: &HashMap<N, HashMap<N, W>>, i, j| {
            dist[&i].get(&j).copied().unwrap_or(W::max_value())
        };

        for k in g.nodes() {
            for i in g.nodes() {
                for j in g.nodes() {
                    let dist_ik = unwrap_dist(&dist, i, k);
                    let dist_kj = unwrap_dist(&dist, k, j);
                    let dist_ij = unwrap_dist(&dist, i, j);
                    if let Some(sum) = dist_ik.checked_add(&dist_kj)
                        && sum < dist_ij
                    {
                        dist.entry(i).and_modify(|ds| {
                            ds.entry(j)
                                .and_modify(|d| *d = sum)
                                .or_insert(sum);
                        });
                        let pred_kj = pred[&k][&j];
                        pred.entry(i).and_modify(|ps| {
                            ps.entry(j)
                                .and_modify(|p| *p = pred_kj)
                                .or_insert(pred_kj);
                        });
                    }
                }
            }
        }

        Self { dist, pred }
    }
}
    \end{minted}
\end{listing}

\subsection{Criação da árvore de caminhos mais curtos}

Para a criação da árvore de menores caminhos, seguiremos uma
abordagem parecida com a implementação do algoritmo de
Floyd-Warshall~(\ref{sec:code_floyd-warshall}). Primeiro definiremos
uma estrutura de retorno, no caso a árvore:

\begin{listing}[H]
  \caption{Estrutura de retorno do algoritmo de criação da árvore de
  caminhos mais curtos}
  \begin{minted}{rust}
struct ShortestPathTree<Node, Weight> {
    node: Node,
    childs: Vec<(Weight, ShortestPathTree<Node, Weight>)>,
}
  \end{minted}
\end{listing}

Novamente, a estrutura é genérica a qualquer tipo de vértice e custo,
identificados por \texttt{Node} e \texttt{Weight}. \texttt{node}
representa o valor de um vértice no nó da árvore e \texttt{childs}
representam os nós filhos da árvore. \texttt{childs} é uma lista de
tuplas de custo e árvore, o custo nessa tupla representa o custo de
percorrer a aresta para o nó filho no segundo argumento da tupla.

Quanto a interface do algoritmo, ele será implementado como o
construtor da estrutura, de maneira similar a interface de
Floyd-Warshall~(\ref{code:interface_floydwarshall}):

\begin{listing}[H]
  \caption{Interface do algoritmo de criação da árvore de caminhos mais curtos}
  \begin{minted}{rust}
impl<N: Node, W: Weight> ShortestPathTree<N, W> {
    fn new(g: &(impl WeightedGraph<N, W> + ?Sized), root: N) -> Self;
}
  \end{minted}
	\end{listing}

Assim como o algoritmo de Floyd-Warshall, o construtor espera um
grafo ponderado, que pode ter seu tamanho conhecido ou não em tempo
de compilação. Além disso, o algoritmo também espera uma raiz
escolhida previamente pelo utilizador da interface. Como em Rust não
há suporte para argumentos com valor padrão, vamos definir uma função
auxiliar que recebe uma árvore e um dicionário de vértices visitados
durante a criação da árvore:

\begin{minted}{rust}
fn build_tree<N: Node, W: Weight>(
    visited: &mut HashSet<N>,
    floyd: &FloydWarshallResult<N, W>,
    tree: &mut ShortestPathTree<N, W>,
    root: N,
) {
    for (&k, &w) in &floyd.dist[&tree.node] {
        if k != tree.node && floyd.pred[&root][&k] == tree.node && visited.insert(k) {
            let mut new_child = ShortestPathTree {
                node: k,
                childs: vec![],
            };
            build_tree(visited, floyd, &mut new_child, root);
            tree.childs.push((w, new_child));
        }
    }
}
\end{minted}

A implementação segue fielmente seu pseudocódigo, a principal
diferença está na condicional \texttt{visited.insert(k)}, que valora
para verdadeiro somente se foi possível inserir o nó \texttt{k} no
dicionário de visitados, ou seja, se não houve repetição.

Por fim, definimos a função principal como:

\begin{listing}[H]
  \caption{Implementação do algoritmo da criação da árvore de menores caminhos}
  \begin{minted}{rust}
fn new(g: &(impl WeightedGraph<N, W> + ?Sized), root: N) -> Self {
    let floyd = g.floyd_warshall();
    let mut visited = HashSet::from([root]);
    let mut tree = ShortestPathTree {
        node: root,
        childs: vec![],
    };
    build_tree(&mut visited, &floyd, &mut tree, root);

    tree
}
  \end{minted}
\end{listing}

\subsection{Algoritmo de Hierholzer}

Para o algoritmo de Hierholzer~(\ref{sec:pseudocode_hierholzer}), inicialmente definimos uma
estrutura, que vai representar o retorno da função:

\begin{listing}[H]
  \caption{Estrutura de resultado do Algoritmo de Hierholzer}
\begin{minted}{rust}
pub struct HierholzerResult<Node> {
    pub path: Vec<Node>,
    pub has_eulerian_path: bool,
    pub has_eulerian_cycle: bool,
}
\end{minted}
\end{listing}

Ela é genérica a qualquer tipo de nó, identificado por \texttt{Node}. 
\texttt{path} representa o caminho euleriano encontrado (se existir), 
\texttt{has\_eulerian\_path} indica se existe um caminho euleriano no grafo,
e \texttt{has\_eulerian\_cycle} indica se existe um ciclo euleriano no grafo.

Após definir a estrutura, definimos a interface do algoritmo como sendo o
construtor dessa estrutura, nomeado de \texttt{new}:
\begin{listing}[H]
  \caption{Interface do algoritmo de Hierholzer}
\begin{minted}{rust}
impl<N: Node> HierholzerResult<N> {
    pub fn new<G: UndirectedGraph<N>>(graph: &G, is_directed: bool) -> Self;
}
\end{minted}
\end{listing}

O algoritmo espera a implementação de um grafo \texttt{graph} que implementa 
a trait \texttt{UndirectedGraph} e um booleano \texttt{is\_directed} indicando 
se o grafo deve ser tratado como direcionado. \texttt{N} é o tipo do nó e é 
restrito pelo traço \texttt{Node}.

Começando a implementação, definimos as estruturas que vão auxiliar no cálculo
e as inicializamos:

\begin{minted}{rust}
let mut out_degree = HashMap::new();
let mut in_degree = HashMap::new();
Self::compute_every_node_degree(graph, &mut out_degree, &mut in_degree);

let (mut start_node, has_eulerian_path, has_eulerian_cycle) =
    Self::check_eulerian_conditions(&out_degree, &in_degree, is_directed);
\end{minted}

Essa inicialização garante que os graus de todos os vértices sejam calculados
e que as condições de Euler sejam verificadas antes de executar o algoritmo
principal. A função \texttt{compute\_every\_node\_degree} calcula os graus de
entrada e saída de todos os vértices, enquanto \texttt{check\_eulerian\_conditions}
determina se o grafo possui ciclo euleriano, caminho euleriano, ambos ou nenhum dos dois.

Em seguida, tratamos os casos especiais onde não existe caminho euleriano ou
onde existe um ciclo trivial:

\begin{minted}{rust}
if !has_eulerian_path && !has_eulerian_cycle {
    return HierholzerResult {
        path: Vec::new(),
        has_eulerian_cycle,
        has_eulerian_path,
    };
} else if has_eulerian_cycle && has_eulerian_path && out_degree.len() == 1 {
    return HierholzerResult {
        path: out_degree.keys().copied().collect::<Vec<N>>(),
        has_eulerian_cycle,
        has_eulerian_path,
    };
}
\end{minted}

O primeiro caso lida com grafos que não possuem nem caminho nem ciclo euleriano,
retornando imediatamente. O segundo caso trata de ciclos triviais em grafos com
apenas um vértice.

Para o algoritmo principal, inicializamos as estruturas de dados necessárias:

\begin{minted}{rust}
let mut stack = Vec::new();
let mut path = Vec::new();
let mut work_graph: G = graph.clone();
\end{minted}

A \texttt{stack} é usada para armazenar vértices durante a exploração, o 
\texttt{path} armazena o caminho final, e \texttt{work\_graph} é uma cópia
do grafo original onde arestas são removidas conforme são visitadas.

Por fim, partimos para o cerne da implementação, o loop principal do algoritmo:

\begin{minted}{rust}
loop {
    if stack.is_empty() && work_graph.neighbors(start_node).count() == 0 {
        break;
    } else if work_graph.neighbors(start_node).count() > 0 {
        stack.push(start_node);

        let next_neighbor = work_graph.neighbors(start_node).next();

        if let Some(neighbor) = next_neighbor {
            if is_directed {
                work_graph.remove_edge(start_node, neighbor);
            } else {
                work_graph.remove_undirected_edge(start_node, neighbor);
            }

            start_node = neighbor;
        }
    } else {
        path.push(start_node);
        if let Some(s) = stack.pop() {
            start_node = s;
        }

        if stack.is_empty() {
            path.push(start_node);
        }
    }
}
path.reverse();
\end{minted}

O algoritmo segue uma estratégia baseada em pilha para construir o caminho
euleriano. Enquanto houver vértices para processar, o algoritmo:

- Se o vértice atual tem arestas incidentes: empilha o vértice e segue uma aresta não visitada.

- Caso contrário: adiciona o vértice ao caminho e retrocede na pilha.

Ao final do loop, o caminho é revertido pois foi construído de trás para frente.

Finalmente, validamos o caminho encontrado e retornamos o resultado:

\begin{minted}{rust}
let total_edges: usize = if is_directed {
    out_degree.values().sum()
} else {
    out_degree.values().sum::<usize>() / 2
};

let valid_path = path.len() == total_edges + 1;

Self {
    path: if valid_path { path } else { Vec::new() },
    has_eulerian_cycle: valid_path && has_eulerian_cycle,
    has_eulerian_path: valid_path && has_eulerian_path,
}
\end{minted}

A validação verifica se o caminho encontrado tem comprimento adequado (número de arestas + 1)
e atualiza os flags indicando a existência de ciclo e caminho euleriano de acordo.
